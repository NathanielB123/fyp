

a normalisation algorithm which depends on
deciding |rw|'s convertability premise cannot work.

The trick is thus: we



 confluence and that whether term

 also need confluence. The new |_‚ä¢_>_| relation definitely

The trick t


The obvious cause of our prior attempt at reduction relation failing was
that 

We clearly need a better reduction relation.






We might attempt to define a small-step reduction relation that corresponds to
this conversion relation.





\section{Normalisation}

We aim to show that this enhanced conversion relation is decidable via
the technique of strong normalisation. We define a small-step reduction
relation:

%if False
\begin{code}
infix 4 _‚ä¢_>_ 
\end{code}
%endif

\begin{code}
-- data _‚ä¢_>_ (Œû : Eqs Œì) : Tm Œì A ‚Üí Tm Œì A ‚Üí Set where
--   -- Computation
--   ‚áíŒ≤   : Œû ‚ä¢ (∆õ t) ¬∑ u  > t [ < u > ]
--   ùîπŒ≤‚ÇÅ  : Œû ‚ä¢ if TT u v  > u 
--   ùîπŒ≤‚ÇÇ  : Œû ‚ä¢ if FF u v  > v
  
--   -- Rewriting
--   eq : EqVar Œû t b ‚Üí Œû ‚ä¢ t > ‚åú b ‚åùùîπ

--   -- Monotonicity
--   ∆õ_   : Œû [ wk ]Eq  ‚ä¢ t‚ÇÅ  > t‚ÇÇ  ‚Üí Œû ‚ä¢ ∆õ t‚ÇÅ       > ∆õ t‚ÇÇ 
--   l¬∑   : Œû           ‚ä¢ t‚ÇÅ  > t‚ÇÇ  ‚Üí Œû ‚ä¢ t‚ÇÅ ¬∑ u     > t‚ÇÇ ¬∑ u
--   ¬∑r   : Œû           ‚ä¢ u‚ÇÅ  > u‚ÇÇ  ‚Üí Œû ‚ä¢ t ¬∑ u‚ÇÅ     > t ¬∑ u‚ÇÇ
--   if‚ÇÅ  : Œû           ‚ä¢ t‚ÇÅ  > t‚ÇÇ  ‚Üí Œû ‚ä¢ if t‚ÇÅ u v  > if t‚ÇÇ u v
--   if‚ÇÇ  : Œû           ‚ä¢ u‚ÇÅ  > u‚ÇÇ  ‚Üí Œû ‚ä¢ if t u‚ÇÅ v  > if t u‚ÇÇ v
--   if‚ÇÉ  : Œû           ‚ä¢ v‚ÇÅ  > v‚ÇÇ  ‚Üí Œû ‚ä¢ if t u v‚ÇÅ  > if t u v‚ÇÇ
\end{code}

% Oh I am very silly
% This reduction relation is nonsense
% The |eq| pre-condition should be |Œû ‚ä¢ t ~ ‚åú b ‚åùùîπ|

% Maybe worth positing the idea of having a reduction which simplifies the
% equational context, and then using the old rule.

% What we can say is that for completes TRSs, the above reduction relation
% is equivalent.

% The completion algorithm can be justified by having |~|-pre is SN, and then 
% only ever
% actually rewriting by completed |TRS|s!

% |~|-pre is SN by existing work
% Normalisation then works by completing every time an equation is introduced,
% and inserting |!| if the equational context is inconsistent.

Note that this recuction relation is not confluent! To stay well-founded, we
do not support rewriting from |TT|/|FF|, but there is nothing stopping 
our equational context from containing ``bad'' equations like |TT ~ FF|.

To normalise terms in arbitrary equational contexts then, we need to first
complete the set of equations into a confluent (ground) TRS, and then proceed
reducing with small-step reduction. Strong normalisation is useful here, 
as taking the fixed point of repeatedly reducing equation LHSs can be
justified by extending reduction order over equational contexts 
lexicographically.

% Todo: we have some design decisions to make here
% We could define a restricted version of |_‚ä¢_>_| which doesn't allow rewriting
% Booleans. Assuming this relation is well-founded (for any equational
% environment), justifying completion is not too hard.

% Alternatively, we could define a predicate on equation sets which guarantees
% they don't have any closed booleans on the LHS. Reduction as-is is SN
% for any such environment. We can then show how this property is decidable
% Normalisation still has to do completion so we can insert |!|s where-ever
% the equational environment is unsound (might be useful to define equality
% collapse early)

% I think I nice way to structure explanations could be: 
% This reduction relation is not SN in presense of Bool equations
% Redundant equations can obviously be removed
% Impossible equations imply equality collapse
% Therefore we can define normalisation as first completing the set of rewrites

\begin{code}
TRS : Eqs Œì ‚Üí Set

sn : TRS Œû ‚Üí ‚àÄ (t : Tm Œì A) ‚Üí SN (Œû ‚ä¢_>_) t

-- TRS defined as:
-- All LHSs irreducible
-- All LHSs neutral
--
-- From this, we can pretty easily derive no overlap/confluence


-- data TRS where
--   ‚Ä¢        : TRS (‚Ä¢ {Œì = Œì})
--   _‚ñ∑_>eq_  : TRS Œû ‚Üí {!!}
\end{code}

%if False
\begin{code}
infix 4 _‚ä¢_>Eq_ _>Eq_ 
\end{code}
%endif

\begin{code}
data _‚ä¢_>Eq_ (Œû : Eqs Œì) : Eqs Œì ‚Üí Eqs Œì ‚Üí Set

_>Eq_ : Eqs Œì ‚Üí Eqs Œì ‚Üí Set
Œ® >Eq Œ¶ = Œ® ‚ä¢ Œ® >Eq Œ¶

data _‚ä¢_>Eq_ Œû where
  vz>  : Œû ‚ä¢ t‚ÇÅ > t‚ÇÇ ‚Üí Œû ‚ä¢ Œ® ‚ñ∑ t‚ÇÅ >eq b >Eq Œ® ‚ñ∑ t‚ÇÇ >eq b
  vs>  : Œû ‚ä¢ Œ®‚ÇÅ >Eq Œ®‚ÇÇ ‚Üí Œû ‚ä¢ t‚ÇÅ ~ t‚ÇÇ ‚Üí Œû ‚ä¢ Œ®‚ÇÅ ‚ñ∑ t‚ÇÅ >eq b >Eq Œ®‚ÇÇ ‚ñ∑ t‚ÇÇ >eq b 

data SNEq (Œû : Eqs Œì) : Eqs Œì ‚Üí Set where
  ‚Ä¢    : SNEq Œû (‚Ä¢ {Œì = Œì})
  _‚ñ∑_  : SNEq Œû Œ® ‚Üí SN (Œû ‚ä¢_>_) t ‚Üí SNEq Œû (Œ® ‚ñ∑ t >eq b)

snEq : TRS Œû ‚Üí ‚àÄ Œ® ‚Üí SN (Œû ‚ä¢_>Eq_) Œ®

sn‚ñ∑   : TRS Œû ‚Üí SN (Œû ‚ä¢_>Eq_) Œ® ‚Üí SN (Œû ‚ä¢_>_) t ‚Üí SN (Œû ‚ä¢_>Eq_) (Œ® ‚ñ∑ t >eq b)
sn‚ñ∑>  : TRS Œû ‚Üí SN (Œû ‚ä¢_>Eq_) Œ®‚ÇÅ ‚Üí SN (Œû ‚ä¢_>_) t‚ÇÅ
      ‚Üí Œû ‚ä¢ Œ®‚ÇÅ ‚ñ∑ t‚ÇÅ >eq b >Eq Œ®‚ÇÇ ‚Üí SN (Œû ‚ä¢_>Eq_) Œ®‚ÇÇ

sn‚ñ∑ Œû·∂ú Œ®·¥¨ t·¥¨ = acc (sn‚ñ∑> Œû·∂ú Œ®·¥¨ t·¥¨)

sn‚ñ∑> Œû·∂ú Œ®·¥¨        (acc t·¥¨)  (vz> t>)     = sn‚ñ∑ Œû·∂ú Œ®·¥¨       (t·¥¨ t>)
sn‚ñ∑> Œû·∂ú (acc Œ®·¥¨)  t·¥¨        (vs> Œ®> t~)  = sn‚ñ∑ Œû·∂ú (Œ®·¥¨ Œ®>)  (sn Œû·∂ú _)

snSNEq  : TRS Œû ‚Üí SNEq Œû Œ® ‚Üí SN (Œû ‚ä¢_>Eq_) Œ®
snSNEq> : TRS Œû ‚Üí SNEq Œû Œ®‚ÇÅ ‚Üí Œû ‚ä¢ Œ®‚ÇÅ >Eq Œ®‚ÇÇ ‚Üí SN (Œû ‚ä¢_>Eq_) Œ®‚ÇÇ

snSNEq Œû·∂ú Œ® = acc (snSNEq> Œû·∂ú Œ®)

-- This sucks - we need to combine structural and SN order here, and I'm not
-- quite sure how to justify it...
snSNEq> Œû·∂ú (Œ® ‚ñ∑ acc t·¥¨)  (vz> t>)    = {! snSNEq Œû·∂ú (Œ® ‚ñ∑ t·¥¨ t>) !}
snSNEq> Œû·∂ú (Œ® ‚ñ∑ t·¥¨)      (vs> Œ®> t~) = sn‚ñ∑ Œû·∂ú (snSNEq> Œû·∂ú Œ® Œ®>) (sn Œû·∂ú _)
\end{code}

We have a reduction relation and potential normalisation algorithm, but 
we of course still need to prove this reduction relation is well-founded.
We do this by picking a slightly larger reduction relation that is 
slightly better-behaved.

\subsection{Spontaneous and Non-Determinsitic Reduction}

\begin{code}
data _>!_ : Tm Œì A ‚Üí Tm Œì A ‚Üí Set where
  eqTT : t >! TT
  eqFF : t >! FF
\end{code}

Proving strong normalisation of spontaneous reduction directly is possible,
but is slightly awkward due to instability under substitutions.
E.g. we have |` i >! TT|, if we then apply the substitution |TT / i| to
both sides, we are left with |TT >! TT|, which is explicitly not allowed
to ensure |TT| stays in normal form.

We therefore consider yet another reduction relation: Non-Deterministic
Reduction. The idea is to allow any |if|-expression to immediately reduce
to the left or right branch (irrespective of the scrutinee).

\begin{code}
data _>ND_ : Tm Œì A ‚Üí Tm Œì A ‚Üí Set where
  ifND‚ÇÅ  : if t u v >ND u
  ifND‚ÇÇ  : if t u v >ND v
\end{code}

It turns out that we can prove that strong normalisation w.r.t.
non-determinsitic reduction implies strong normalisation w.r.t. spontaneous
reduction. In fact, we can prove this result inside untyped lambda calculus
(removing the |ùîπ|-typed condition on spontaneous reduction. We will prove
this now.










With Œ≤-equality alone, we quickly get stuck if the scrutinee is a variable.
E.g. equations like |if t u u ~ u| or |if t t u ~ TT|. As mentioned in
(TODO REFERENCE RELATED WORK), strict Œ∑ for Booleans can make such
such equations derivable.

\begin{code}
ùîπŒ∑ : u [ < t > ] ~ if t (u [ < TT > ]) (v [ < FF > ])
\end{code}

However, I claim that |ùîπŒ∑| is too strong. Existing normalisation algorithms
all rely on effectively splitting on every Boolean neutral, requiring
re-normalising on the order of $2^n$ times, where $n$ is the number 
of distinct neutral Boolean subterms. 

%TODO Finish writing this
















But is not quite what we want: |_‚ä¢_~_| should not only identify
terms modulo a fixed set of Boolean equations. We should be introducing
new equations in the branches of each |if| expression, i.e.

\begin{spec}
  if  : Œû ‚ä¢ t‚ÇÅ ~ t‚ÇÇ ‚Üí Œû ‚ñ∑ t‚ÇÅ >eq true ‚ä¢ u‚ÇÅ ~ u‚ÇÇ ‚Üí Œû ‚ñ∑ t‚ÇÅ >eq false ‚ä¢ v‚ÇÅ ~ v‚ÇÇ
      ‚Üí Œû ‚ä¢ if t‚ÇÅ u‚ÇÅ v‚ÇÅ ~ if t‚ÇÇ u‚ÇÇ v‚ÇÇ
\end{spec}

We somewhat arbitrarily insert |t‚ÇÅ|, rather than |t‚ÇÇ|, into the equational
context. From stability of conversion over
weakening the
equational context
(i.e. adding new equations), we find the
choice here is ultimately irrelevant:


\begin{code}
data WkEqs {Œì} : Eqs Œì ‚Üí Eqs Œì ‚Üí Set where
  Œµ        : WkEqs ‚Ä¢ ‚Ä¢
  _‚Å∫_>eq_  : WkEqs Œ¶ Œ® ‚Üí Tm Œì ùîπ ‚Üí Bool ‚Üí WkEqs (Œ¶ ‚ñ∑ t >eq b) (Œ® ‚ñ∑ t >eq b)

wkeq : WkEqs (Œû ‚ñ∑ t >eq b) Œû

_[_]Wk~ : Œ® ‚ä¢ t‚ÇÅ ~ t‚ÇÇ ‚Üí WkEqs Œ¶ Œ® ‚Üí Œ¶ ‚ä¢ t‚ÇÅ ~ t‚ÇÇ

swapEqVar  : Œû ‚ä¢ t‚ÇÅ ~ t‚ÇÇ ‚Üí EqVar (Œû ‚ñ∑ t‚ÇÅ >eq true) u b 
           ‚Üí Œû ‚ñ∑ t‚ÇÇ >eq true ‚ä¢ u ~ ‚åú b ‚åùùîπ
swapEqVar t~ ez      = t~ [ wkeq ]Wk~ ‚àô~ eq ez
swapEqVar t~ (es e)  = eq (es e)
\end{code}

%if False
\begin{code}
_[_]~ : Œû ‚ä¢ t‚ÇÅ ~ t‚ÇÇ ‚Üí ‚àÄ (Œ¥ : Tms[ q ] Œî Œì) ‚Üí Œû [ Œ¥ ]Eq ‚ä¢ t‚ÇÅ [ Œ¥ ] ~ t‚ÇÇ [ Œ¥ ]


-- I think we need an IH that equational contexts are equivalent...

swapEq : Œû ‚ä¢ t‚ÇÅ ~ t‚ÇÇ ‚Üí (Œû ‚ñ∑ t‚ÇÅ >eq true) ‚ä¢ u‚ÇÅ ~ u‚ÇÇ
       ‚Üí (Œû ‚ñ∑ t‚ÇÇ >eq true) ‚ä¢ u‚ÇÅ ~ u‚ÇÇ
swapEq t~ (eq e)            = swapEqVar t~ e
swapEq t~ (∆õ u~)            = ∆õ swapEq (t~ [ wk ]~) u~

swapEq t~ (if u‚ÇÅ~ u‚ÇÇ~ u‚ÇÉ~)  = {!!}
  -- if (swapEq t~ u‚ÇÅ~) (swapEq {!   !} {!  u‚ÇÇ~ !}) {!!}

swapEq t~ ‚áíŒ≤ = {!   !}
swapEq t~ ùîπŒ≤‚ÇÅ = {!   !}
swapEq t~ ùîπŒ≤‚ÇÇ = {!   !}
swapEq t~ rfl~ = {!   !}
swapEq t~ (sym~ u~) = {!   !}
swapEq t~ (u~‚ÇÅ ‚àô~ u~‚ÇÇ) = {!   !}
swapEq t~ (u~‚ÇÅ ¬∑ u~‚ÇÇ) = {!   !}
\end{code}
%endif

% TODO Find a place for these definitions

